{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enhanced-payday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-3IA1UT8:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x21fc3a68340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tracked-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "     .config(\"spark.driver.memory\",\"16G\")\\\n",
    "     .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "     .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "     .appName(\"Spark NLP\") \\\n",
    "     .config(\"spark.jars.packages\", \n",
    "             \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.3.5\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sharing-library",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nares\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assumed-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "eng_stopwords = stopwords.words('english')\n",
    "eng_stopwords.append('xxxx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "physical-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import (Tokenizer, Normalizer, \n",
    "                                LemmatizerModel, StopWordsCleaner)\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-disclosure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# note normalizer defaults to changing all words to lowercase.\n",
    "# Use .setLowercase(False) to maintain input case.\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols(['token']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setLowercase(True)\n",
    "\n",
    "# note that lemmatizer needs a dictionary. So I used the pre-trained\n",
    "# model (note that it defaults to english)\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('lemma') \\\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols(['lemma']) \\\n",
    "    .setOutputCol('clean_lemma') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "# finisher converts tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols(['clean_lemma']) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "facial-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline() \\\n",
    "     .setStages([\n",
    "           documentAssembler,\n",
    "           tokenizer,\n",
    "           normalizer,\n",
    "           lemmatizer,\n",
    "           stopwords_cleaner,\n",
    "           finisher\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "persistent-passport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(text='Two people killed in fiery Tesla crash with no one driving', document=[Row(annotatorType='document', begin=0, end=57, result='Two people killed in fiery Tesla crash with no one driving', metadata={'sentence': '0'}, embeddings=[])], token=[Row(annotatorType='token', begin=0, end=2, result='Two', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=9, result='people', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='killed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=25, result='fiery', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=31, result='Tesla', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=37, result='crash', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=39, end=42, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=44, end=45, result='no', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=47, end=49, result='one', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=51, end=57, result='driving', metadata={'sentence': '0'}, embeddings=[])], normalized=[Row(annotatorType='token', begin=0, end=2, result='two', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=9, result='people', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='killed', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=25, result='fiery', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=31, result='tesla', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=37, result='crash', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=39, end=42, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=44, end=45, result='no', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=47, end=49, result='one', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=51, end=57, result='driving', metadata={'sentence': '0'}, embeddings=[])], lemma=[Row(annotatorType='token', begin=0, end=2, result='two', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=9, result='people', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='kill', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=18, end=19, result='in', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=25, result='fiery', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=31, result='tesla', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=37, result='crash', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=39, end=42, result='with', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=44, end=45, result='no', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=47, end=49, result='one', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=51, end=57, result='drive', metadata={'sentence': '0'}, embeddings=[])], clean_lemma=[Row(annotatorType='token', begin=0, end=2, result='two', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=4, end=9, result='people', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=11, end=16, result='kill', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=21, end=25, result='fiery', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=27, end=31, result='tesla', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=33, end=37, result='crash', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=47, end=49, result='one', metadata={'sentence': '0'}, embeddings=[]), Row(annotatorType='token', begin=51, end=57, result='drive', metadata={'sentence': '0'}, embeddings=[])], finished_clean_lemma=['two', 'people', 'kill', 'fiery', 'tesla', 'crash', 'one', 'drive'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "df = spark.read.csv('twitter.csv',\n",
    "                     inferSchema='true', header='true')\n",
    "data = df.select('text')\n",
    "                                                               \n",
    "# transform text with the pipeline\n",
    "equifax = pipeline.fit(data).transform(data)\n",
    "equifax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "permanent-cheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>normalized</th>\n",
       "      <th>lemma</th>\n",
       "      <th>clean_lemma</th>\n",
       "      <th>finished_clean_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>[(document, 0, -1, , {'sentence': '0'}, [])]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @lorakolodny: Two men dead after Tesla cras...</td>\n",
       "      <td>[(document, 0, 138, RT @lorakolodny: Two men d...</td>\n",
       "      <td>[(token, 0, 1, RT, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[rt, lorakolodny, two, man, dead, tesla, crash...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @elonmusk: Tesla with Autopilot engaged now...</td>\n",
       "      <td>[(document, 0, 112, RT @elonmusk: Tesla with A...</td>\n",
       "      <td>[(token, 0, 1, RT, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[rt, elonmusk, tesla, autopilot, engage, appro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @JadeRhysThomas: Because my Transmasc artis...</td>\n",
       "      <td>[(document, 0, 139, RT @JadeRhysThomas: Becaus...</td>\n",
       "      <td>[(token, 0, 1, RT, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[(token, 0, 1, rt, {'sentence': '0'}, []), (to...</td>\n",
       "      <td>[rt, jaderhysthomas, transmasc, artist, friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Two people killed in fiery Tesla crash with no...   \n",
       "1                                               None   \n",
       "2  RT @lorakolodny: Two men dead after Tesla cras...   \n",
       "3  RT @elonmusk: Tesla with Autopilot engaged now...   \n",
       "4  RT @JadeRhysThomas: Because my Transmasc artis...   \n",
       "\n",
       "                                            document  \\\n",
       "0  [(document, 0, 57, Two people killed in fiery ...   \n",
       "1       [(document, 0, -1, , {'sentence': '0'}, [])]   \n",
       "2  [(document, 0, 138, RT @lorakolodny: Two men d...   \n",
       "3  [(document, 0, 112, RT @elonmusk: Tesla with A...   \n",
       "4  [(document, 0, 139, RT @JadeRhysThomas: Becaus...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "1                                                 []   \n",
       "2  [(token, 0, 1, RT, {'sentence': '0'}, []), (to...   \n",
       "3  [(token, 0, 1, RT, {'sentence': '0'}, []), (to...   \n",
       "4  [(token, 0, 1, RT, {'sentence': '0'}, []), (to...   \n",
       "\n",
       "                                          normalized  \\\n",
       "0  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1                                                 []   \n",
       "2  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "3  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "4  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1                                                 []   \n",
       "2  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "3  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "4  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "\n",
       "                                         clean_lemma  \\\n",
       "0  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1                                                 []   \n",
       "2  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "3  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "4  [(token, 0, 1, rt, {'sentence': '0'}, []), (to...   \n",
       "\n",
       "                                finished_clean_lemma  \n",
       "0  [two, people, kill, fiery, tesla, crash, one, ...  \n",
       "1                                                 []  \n",
       "2  [rt, lorakolodny, two, man, dead, tesla, crash...  \n",
       "3  [rt, elonmusk, tesla, autopilot, engage, appro...  \n",
       "4  [rt, jaderhysthomas, transmasc, artist, friend...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#equifax = equifax.toPandas()\n",
    "equifax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "healthy-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "equifax_words = equifax.withColumn('exploded_text', \n",
    "                               explode(col('finished_clean_lemma')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "raising-dialogue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text',\n",
       " 'document',\n",
       " 'token',\n",
       " 'normalized',\n",
       " 'lemma',\n",
       " 'clean_lemma',\n",
       " 'finished_clean_lemma',\n",
       " 'exploded_text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equifax_words.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "flush-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>document</th>\n",
       "      <th>token</th>\n",
       "      <th>normalized</th>\n",
       "      <th>lemma</th>\n",
       "      <th>clean_lemma</th>\n",
       "      <th>finished_clean_lemma</th>\n",
       "      <th>exploded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>fiery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 57, Two people killed in fiery ...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 105, Two people killed in fiery...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 105, Two people killed in fiery...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 105, Two people killed in fiery...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>drive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 105, Two people killed in fiery...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>httpstcojfawp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Two people killed in fiery Tesla crash with no...</td>\n",
       "      <td>[(document, 0, 105, Two people killed in fiery...</td>\n",
       "      <td>[(token, 0, 2, Two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[(token, 0, 2, two, {'sentence': '0'}, []), (t...</td>\n",
       "      <td>[two, people, kill, fiery, tesla, crash, one, ...</td>\n",
       "      <td>httpstcodvfqajq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>534 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Two people killed in fiery Tesla crash with no...   \n",
       "1    Two people killed in fiery Tesla crash with no...   \n",
       "2    Two people killed in fiery Tesla crash with no...   \n",
       "3    Two people killed in fiery Tesla crash with no...   \n",
       "4    Two people killed in fiery Tesla crash with no...   \n",
       "..                                                 ...   \n",
       "529  Two people killed in fiery Tesla crash with no...   \n",
       "530  Two people killed in fiery Tesla crash with no...   \n",
       "531  Two people killed in fiery Tesla crash with no...   \n",
       "532  Two people killed in fiery Tesla crash with no...   \n",
       "533  Two people killed in fiery Tesla crash with no...   \n",
       "\n",
       "                                              document  \\\n",
       "0    [(document, 0, 57, Two people killed in fiery ...   \n",
       "1    [(document, 0, 57, Two people killed in fiery ...   \n",
       "2    [(document, 0, 57, Two people killed in fiery ...   \n",
       "3    [(document, 0, 57, Two people killed in fiery ...   \n",
       "4    [(document, 0, 57, Two people killed in fiery ...   \n",
       "..                                                 ...   \n",
       "529  [(document, 0, 105, Two people killed in fiery...   \n",
       "530  [(document, 0, 105, Two people killed in fiery...   \n",
       "531  [(document, 0, 105, Two people killed in fiery...   \n",
       "532  [(document, 0, 105, Two people killed in fiery...   \n",
       "533  [(document, 0, 105, Two people killed in fiery...   \n",
       "\n",
       "                                                 token  \\\n",
       "0    [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "1    [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "2    [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "3    [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "4    [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "..                                                 ...   \n",
       "529  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "530  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "531  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "532  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "533  [(token, 0, 2, Two, {'sentence': '0'}, []), (t...   \n",
       "\n",
       "                                            normalized  \\\n",
       "0    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "2    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "3    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "4    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "..                                                 ...   \n",
       "529  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "530  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "531  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "532  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "533  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "\n",
       "                                                 lemma  \\\n",
       "0    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "2    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "3    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "4    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "..                                                 ...   \n",
       "529  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "530  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "531  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "532  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "533  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "\n",
       "                                           clean_lemma  \\\n",
       "0    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "1    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "2    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "3    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "4    [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "..                                                 ...   \n",
       "529  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "530  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "531  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "532  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "533  [(token, 0, 2, two, {'sentence': '0'}, []), (t...   \n",
       "\n",
       "                                  finished_clean_lemma    exploded_text  \n",
       "0    [two, people, kill, fiery, tesla, crash, one, ...              two  \n",
       "1    [two, people, kill, fiery, tesla, crash, one, ...           people  \n",
       "2    [two, people, kill, fiery, tesla, crash, one, ...             kill  \n",
       "3    [two, people, kill, fiery, tesla, crash, one, ...            fiery  \n",
       "4    [two, people, kill, fiery, tesla, crash, one, ...            tesla  \n",
       "..                                                 ...              ...  \n",
       "529  [two, people, kill, fiery, tesla, crash, one, ...            crash  \n",
       "530  [two, people, kill, fiery, tesla, crash, one, ...              one  \n",
       "531  [two, people, kill, fiery, tesla, crash, one, ...            drive  \n",
       "532  [two, people, kill, fiery, tesla, crash, one, ...    httpstcojfawp  \n",
       "533  [two, people, kill, fiery, tesla, crash, one, ...  httpstcodvfqajq  \n",
       "\n",
       "[534 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equifax_words1 = equifax_words.toPandas()\n",
    "equifax_words1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_clean_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "integral-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = equifax_words.groupby('exploded_text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "extensive-checkout",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exploded_text</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>art</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hope</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>giga</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>httpstcotuualcafd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transaction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>almost</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>big</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>fiery</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>capaci</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>street</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>325 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         exploded_text  count\n",
       "0                  art      4\n",
       "1                 hope      5\n",
       "2                 giga      2\n",
       "3    httpstcotuualcafd      1\n",
       "4          transaction      1\n",
       "..                 ...    ...\n",
       "320             almost      1\n",
       "321                big      1\n",
       "322              fiery      7\n",
       "323             capaci      1\n",
       "324             street      1\n",
       "\n",
       "[325 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_pd = counts.toPandas()\n",
    "counts_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pretty-vector",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_pd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "representative-heart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'art': 4,\n",
       " 'hope': 5,\n",
       " 'giga': 2,\n",
       " 'httpstcotuualcafd': 1,\n",
       " 'transaction': 1,\n",
       " 'positive': 1,\n",
       " 'lmfao': 1,\n",
       " 'explain': 1,\n",
       " 'equal': 1,\n",
       " 'alexroy': 1,\n",
       " 'amp': 1,\n",
       " 'film': 1,\n",
       " 'ready': 1,\n",
       " 'elonmusk': 5,\n",
       " 'day': 1,\n",
       " 'perc': 1,\n",
       " 'even': 1,\n",
       " 'happen': 1,\n",
       " 'joseantonio': 1,\n",
       " 'two': 9,\n",
       " 'local': 1,\n",
       " 'odd': 1,\n",
       " 'loan': 1,\n",
       " 'buy': 3,\n",
       " 'promise': 1,\n",
       " 'donate': 2,\n",
       " 'cryptos': 1,\n",
       " 'rise': 1,\n",
       " 'localbadde': 1,\n",
       " 'g': 1,\n",
       " 'low': 2,\n",
       " 'safety': 2,\n",
       " 'teslectrics': 1,\n",
       " 'venture': 1,\n",
       " 'heck': 1,\n",
       " 'hiphopcultnft': 1,\n",
       " 'capable': 2,\n",
       " 'month': 1,\n",
       " 'httpstcoxnrdwyysff': 1,\n",
       " 'elongateog': 1,\n",
       " 'hand': 1,\n",
       " 'neto': 1,\n",
       " 'commission': 2,\n",
       " 'new': 4,\n",
       " 'success': 1,\n",
       " 'jose': 1,\n",
       " 'amazon': 1,\n",
       " 'simultaneously': 1,\n",
       " 'man': 3,\n",
       " 'efficiencylast': 1,\n",
       " 'netflix': 1,\n",
       " 'vincent': 1,\n",
       " 'httpstcokeszghrpy': 1,\n",
       " 'ark': 1,\n",
       " 'minefox': 1,\n",
       " 'ppp': 1,\n",
       " 'rt': 19,\n",
       " 'stay': 1,\n",
       " 'auto': 1,\n",
       " 'chamber': 1,\n",
       " 'friend': 2,\n",
       " 'optimistic': 1,\n",
       " 'httpstcovzhqtgg': 1,\n",
       " 'rcnne': 1,\n",
       " 'soon': 1,\n",
       " 'money': 2,\n",
       " 'halfbaked': 2,\n",
       " 'wedding': 1,\n",
       " 'safe': 1,\n",
       " 'freezer': 1,\n",
       " 'berlin': 1,\n",
       " 'one': 9,\n",
       " 'popularity': 1,\n",
       " 'average': 3,\n",
       " 'texas': 4,\n",
       " 'seat': 5,\n",
       " 'teslaadri': 2,\n",
       " 'cameraonly': 2,\n",
       " 'httpstcogbcqelh': 1,\n",
       " 'f': 1,\n",
       " 'person': 1,\n",
       " 'spring': 2,\n",
       " 'tell': 1,\n",
       " 'fraudulent': 1,\n",
       " 'reignit': 1,\n",
       " 'read': 1,\n",
       " 'teslas': 1,\n",
       " 'say': 3,\n",
       " 'religious': 1,\n",
       " 'trettonkelevra': 1,\n",
       " 'wont': 1,\n",
       " 'take': 1,\n",
       " 'future': 1,\n",
       " 'reporter': 1,\n",
       " 'nonautopilot': 1,\n",
       " 'k': 1,\n",
       " 'mile': 1,\n",
       " 'comparison': 1,\n",
       " 'kimpaquette': 1,\n",
       " 'iot': 1,\n",
       " 'digiltable': 1,\n",
       " 'plus': 1,\n",
       " 'light': 1,\n",
       " 'chinchillazllla': 1,\n",
       " 'jungyoonlim': 1,\n",
       " 'elons': 1,\n",
       " 'great': 1,\n",
       " 'speculawyer': 1,\n",
       " 'elon': 1,\n",
       " 'folio': 1,\n",
       " 'fbi': 1,\n",
       " 'slye': 1,\n",
       " 'ice': 1,\n",
       " 'pm': 1,\n",
       " 'tweetermeyer': 3,\n",
       " 'job': 1,\n",
       " 'close': 1,\n",
       " 'mblnft': 1,\n",
       " 'tree': 1,\n",
       " 'tesla': 31,\n",
       " 'fsd': 2,\n",
       " 'family': 2,\n",
       " 'dollar': 1,\n",
       " 'definitely': 1,\n",
       " 'roll': 2,\n",
       " 'httpstcojmgiuosr': 1,\n",
       " 'active': 1,\n",
       " 'life': 2,\n",
       " 'thedailybeast': 1,\n",
       " 'glass': 2,\n",
       " 'engage': 2,\n",
       " 'gladiatoroffice': 1,\n",
       " 'hound': 1,\n",
       " 'im': 2,\n",
       " 'nothing': 2,\n",
       " 'ok': 1,\n",
       " 'httpstcodvfqajq': 1,\n",
       " 'official': 2,\n",
       " 'httpstcopxsaqimcw': 1,\n",
       " 'sell': 1,\n",
       " 'people': 8,\n",
       " 'pastor': 1,\n",
       " 'mouthpiece': 1,\n",
       " 'passenger': 1,\n",
       " 'abu': 1,\n",
       " 'use': 2,\n",
       " 'wheel': 1,\n",
       " 'cost': 2,\n",
       " 'diffranklin': 1,\n",
       " 'httpstcouwihsjrrf': 1,\n",
       " 'refer': 1,\n",
       " 'btc': 1,\n",
       " 'energysmartohio': 1,\n",
       " 'buckle': 1,\n",
       " 'approach': 2,\n",
       " 'belt': 1,\n",
       " 'data': 2,\n",
       " 'ktequilashots': 1,\n",
       " 'aligoudarzi': 1,\n",
       " 'breakrooms': 1,\n",
       " 'imosmaller': 1,\n",
       " 'te': 1,\n",
       " 'module': 1,\n",
       " 'media': 1,\n",
       " 'artkostantinov': 1,\n",
       " 'social': 1,\n",
       " 'drive': 10,\n",
       " 'spagritty': 1,\n",
       " 'modem': 1,\n",
       " 'second': 1,\n",
       " 'advocate': 1,\n",
       " 'coil': 1,\n",
       " 'brianhooper': 1,\n",
       " 'httpstcojfawp': 1,\n",
       " 'covenant': 1,\n",
       " 'actual': 1,\n",
       " 'dip': 2,\n",
       " 'entire': 1,\n",
       " 'block': 1,\n",
       " 'transmasc': 2,\n",
       " 'consider': 1,\n",
       " 'artist': 2,\n",
       " 'ask': 1,\n",
       " 'see': 2,\n",
       " 'ill': 1,\n",
       " 'high': 1,\n",
       " 'win': 1,\n",
       " 'correct': 1,\n",
       " 'hazard': 1,\n",
       " 'tsla': 1,\n",
       " 'spread': 1,\n",
       " 'million': 2,\n",
       " 'warehouse': 1,\n",
       " 'transformertube': 1,\n",
       " 'gme': 1,\n",
       " 'dc': 1,\n",
       " 'thatcherulrich': 1,\n",
       " 'dubstepsex': 1,\n",
       " 'bankrupt': 1,\n",
       " 'make': 1,\n",
       " 'road': 1,\n",
       " 'dear': 4,\n",
       " 'httpstcoyprgarlf': 1,\n",
       " 'work': 4,\n",
       " 'cus': 1,\n",
       " 'follow': 1,\n",
       " 'die': 2,\n",
       " 'like': 7,\n",
       " 'cop': 1,\n",
       " 'lie': 1,\n",
       " 'sound': 1,\n",
       " 'usual': 1,\n",
       " 'thejoshestyoung': 1,\n",
       " 'driver': 3,\n",
       " 'xaca': 1,\n",
       " 'force': 1,\n",
       " 'every': 1,\n",
       " 'houston': 1,\n",
       " 'manicmarge': 1,\n",
       " 'away': 1,\n",
       " 'setup': 1,\n",
       " 'econmarshall': 1,\n",
       " 'look': 1,\n",
       " 'hangzhou': 1,\n",
       " 'mall': 1,\n",
       " 'prototesla': 1,\n",
       " 'yesterday': 1,\n",
       " 'downstem': 1,\n",
       " 'case': 1,\n",
       " 'xnewdata': 1,\n",
       " 'wisconsin': 2,\n",
       " 'store': 1,\n",
       " 'ribeiro': 1,\n",
       " 'mkbhd': 1,\n",
       " 'doesnt': 1,\n",
       " 'jaderhysthomas': 2,\n",
       " 'tragictesla': 1,\n",
       " 'experiment': 2,\n",
       " 'lorakolodny': 1,\n",
       " 'dad': 1,\n",
       " 'newyorkfathead': 1,\n",
       " 'unless': 1,\n",
       " 'uber': 1,\n",
       " 'yes': 1,\n",
       " 'accident': 3,\n",
       " 'libcrusher': 1,\n",
       " 'time': 3,\n",
       " 'least': 1,\n",
       " 'deitaone': 1,\n",
       " 'well': 1,\n",
       " 'crash': 10,\n",
       " 'vehicle': 3,\n",
       " 'teslaowls': 1,\n",
       " 'thousands': 1,\n",
       " 'try': 1,\n",
       " 'get': 2,\n",
       " 'nag': 1,\n",
       " 'prettycritical': 1,\n",
       " 'nobody': 2,\n",
       " 'bevedoni': 1,\n",
       " 'chance': 2,\n",
       " 'nikolatruth': 1,\n",
       " 'httpstcowteruos': 1,\n",
       " 'robotaxis': 1,\n",
       " 'would': 1,\n",
       " 'httpstcohypmqsbj': 1,\n",
       " 'agree': 2,\n",
       " 'tobbber': 1,\n",
       " 'httpstcokkjyfeskw': 1,\n",
       " 'bullspacman': 1,\n",
       " 'company': 1,\n",
       " 'zezinho': 1,\n",
       " 'theverge': 1,\n",
       " 'fatal': 1,\n",
       " 'httpstcozpfptyaod': 1,\n",
       " 'china': 1,\n",
       " 'voltage': 1,\n",
       " 'government': 1,\n",
       " 'httpstcohkdzkrtzul': 1,\n",
       " 'mrbeast': 1,\n",
       " 'httpstcoqwboeapmg': 1,\n",
       " 'teslaisatrex': 1,\n",
       " 'car': 4,\n",
       " 'acquire': 1,\n",
       " 'httpstcozzmtjyiwd': 1,\n",
       " 'base': 2,\n",
       " 'httpstcoggvymh': 1,\n",
       " 'rock': 1,\n",
       " 'httpstcolrowufeep': 1,\n",
       " 'drop': 2,\n",
       " 'musk': 2,\n",
       " 'know': 1,\n",
       " 'anyway': 1,\n",
       " 'regulation': 1,\n",
       " 'side': 1,\n",
       " 'teslaelon': 1,\n",
       " 'point': 1,\n",
       " 'ultimately': 1,\n",
       " 'assumption': 1,\n",
       " 'first': 2,\n",
       " 'verge': 1,\n",
       " 'kill': 4,\n",
       " 'require': 1,\n",
       " 'kprc': 1,\n",
       " 'antonio': 1,\n",
       " 'autopilot': 5,\n",
       " 'autonomy': 2,\n",
       " 'bowl': 1,\n",
       " 'many': 1,\n",
       " 'nftfreaks': 1,\n",
       " 'distance': 1,\n",
       " 'also': 1,\n",
       " 'collector': 4,\n",
       " 'keep': 3,\n",
       " 'dead': 5,\n",
       " 'less': 1,\n",
       " 'lb': 2,\n",
       " 'via': 1,\n",
       " 'google': 1,\n",
       " 'likely': 1,\n",
       " 'almost': 1,\n",
       " 'big': 1,\n",
       " 'fiery': 7,\n",
       " 'capaci': 1,\n",
       " 'street': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{counts_pd.loc[i, 'exploded_text']: counts_pd.loc[i, 'count'] for i in range(counts_pd.shape[0])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
